{
 "cells": [
  {
   "metadata": {},
   "cell_type": "raw",
   "source": "{\"cells\":[{\"cell_type\":\"markdown\",\"id\":\"1a5370f4\",\"metadata\":{\"id\":\"1a5370f4\"},\"source\":[\"# Simple Reflex vs Model-Based Reflex Agents\\n\",\"\\n\",\"Before you begin, make sure to download the file and save with your name, e.g. LASTNAME_VaccumWorld_ReflexAgents\\n\",\"\\n\",\"\\n\",\"You will implement and compare two agent programs in the classic **Vacuum World**.\\n\",\"\\n\",\"## Learning goals\\n\",\"- Implement a **simple reflex agent** using condition–action rules.\\n\",\"- Implement a **model-based reflex agent** using **internal state**.\\n\",\"- Run experiments and compare performance across random environments.\\n\",\"- Explain *why* internal state matters, especially when the percept is incomplete.\\n\",\"\\n\",\"---\\n\",\"\\n\",\"## What you will submit\\n\",\"- This notebook with all TODOs completed.\\n\",\"- Short written answers in the marked **Answer** cells.\\n\",\"\\n\",\"Acknowledgement: Some aspects of the notebook was co-written using ChatGTP 5.2 (specifically a few code writeup and formatting); This notebook was prepared following direct discussions on Agent programs in the class materials (please see Agent program discussions before you begin).\\n\",\"\\n\",\"\\n\"]},{\"cell_type\":\"markdown\",\"id\":\"8984e445\",\"metadata\":{\"id\":\"8984e445\"},\"source\":[\"## 0. Setup\\n\",\"\\n\",\"Run the next cell to import libraries. Do not add new external packages.\\n\"]},{\"cell_type\":\"code\",\"execution_count\":null,\"id\":\"a2b1cbb2\",\"metadata\":{\"id\":\"a2b1cbb2\"},\"outputs\":[],\"source\":[\"import random\\n\",\"from dataclasses import dataclass\\n\",\"from typing import Dict, Tuple, Optional, List\\n\",\"\\n\",\"# Reproducibility\\n\",\"random.seed(0)\\n\"]},{\"cell_type\":\"markdown\",\"id\":\"5ee1f897\",\"metadata\":{\"id\":\"5ee1f897\"},\"source\":[\"## 1. Vacuum World Environment\\n\",\"\\n\",\"The environment has:\\n\",\"- two locations: `Left`, `Right`\\n\",\"- each location is `Clean` or `Dirty`\\n\",\"- the agent occupies one location at a time\\n\",\"\\n\",\"A **percept** is a tuple `(location, status_of_current_square)`.\\n\",\"\\n\",\"Run the next cell to define the environment.\\n\"]},{\"cell_type\":\"code\",\"execution_count\":null,\"id\":\"5c74674e\",\"metadata\":{\"id\":\"5c74674e\"},\"outputs\":[],\"source\":[\"Location = str  # 'Left' or 'Right'\\n\",\"Status   = str    # 'Clean' or 'Dirty'\\n\",\"Action   = str    # 'Left', 'Right', 'Suck', 'NoOp'\\n\",\"Percept  = Tuple[Location, Status]\\n\",\"\\n\",\"@dataclass\\n\",\"class VacuumWorld:\\n\",\"    dirt: Dict[Location, Status]\\n\",\"    agent_loc: Location\\n\",\"    step_count: int = 0\\n\",\"    max_steps: int = 20\\n\",\"\\n\",\"    def percept(self) -> Percept:\\n\",\"        return (self.agent_loc, self.dirt[self.agent_loc])\\n\",\"\\n\",\"    def step(self, action: Action) -> Tuple[Percept, float, bool]:\\n\",\"        \\\"\\\"\\\"Take one step.\\n\",\"\\n\",\"        Returns:\\n\",\"          new_percept, reward, done\\n\",\"        Reward design (simple):\\n\",\"          - +1 for each clean square (max 2) each time step\\n\",\"          - minus 0.1 cost for movement actions\\n\",\"        \\\"\\\"\\\"\\n\",\"        self.step_count += 1\\n\",\"\\n\",\"        # Transition dynamics\\n\",\"        if action == \\\"Suck\\\":\\n\",\"            self.dirt[self.agent_loc] = \\\"Clean\\\"\\n\",\"        elif action == \\\"Left\\\":\\n\",\"            self.agent_loc = \\\"Left\\\"\\n\",\"        elif action == \\\"Right\\\":\\n\",\"            self.agent_loc = \\\"Right\\\"\\n\",\"        elif action == \\\"NoOp\\\":\\n\",\"            pass\\n\",\"        else:\\n\",\"            raise ValueError(f\\\"Unknown action: {action}\\\")\\n\",\"\\n\",\"        clean_count = sum(1 for s in self.dirt.values() if s == \\\"Clean\\\")\\n\",\"        move_cost = 0.1 if action in (\\\"Left\\\", \\\"Right\\\") else 0.0\\n\",\"        reward = float(clean_count) - move_cost\\n\",\"\\n\",\"        done = self.step_count >= self.max_steps\\n\",\"        return self.percept(), reward, done\\n\",\"\\n\",\"\\n\",\"def random_world(p_dirty: float = 0.5, start_loc: Optional[Location] = None, max_steps: int = 20) -> VacuumWorld:\\n\",\"    \\\"\\\"\\\"Create a random vacuum world.\\\"\\\"\\\"\\n\",\"    dirt = {\\n\",\"        \\\"Left\\\":  \\\"Dirty\\\" if random.random() < p_dirty else \\\"Clean\\\",\\n\",\"        \\\"Right\\\": \\\"Dirty\\\" if random.random() < p_dirty else \\\"Clean\\\",\\n\",\"    }\\n\",\"    if start_loc is None:\\n\",\"        start_loc = random.choice([\\\"Left\\\", \\\"Right\\\"])\\n\",\"    return VacuumWorld(dirt=dirt, agent_loc=start_loc, max_steps=max_steps)\\n\"]},{\"cell_type\":\"markdown\",\"id\":\"82263d9e\",\"metadata\":{\"id\":\"82263d9e\"},\"source\":[\"## 2. TODO: Simple Reflex Agent (Condition–Action Rules)\\n\",\"\\n\",\"Implement the classic simple reflex logic:\\n\",\"\\n\",\"- If current square is `Dirty` → `Suck`\\n\",\"- Else move to the other square (`Left` ↔ `Right`)\\n\",\"\\n\",\"**Important:** This agent ignores percept history. It reacts to the current percept only.\\n\"]},{\"cell_type\":\"code\",\"execution_count\":null,\"id\":\"58e195a5\",\"metadata\":{\"id\":\"58e195a5\"},\"outputs\":[],\"source\":[\"def simple_reflex_agent(percept: Percept) -> Action:\\n\",\"    \\\"\\\"\\\"Return an action based only on the current percept.\\n\",\"\\n\",\"    TODO:\\n\",\"    - Implement condition–action rules described above.\\n\",\"    \\\"\\\"\\\"\\n\",\"    # TODO: unpack percept\\n\",\"    # TODO: if dirty -> 'Suck'\\n\",\"    # TODO: else move to other side\\n\",\"    raise NotImplementedError\\n\"]},{\"cell_type\":\"markdown\",\"id\":\"12127e7d\",\"metadata\":{\"id\":\"12127e7d\"},\"source\":[\"### Quick tests for your simple reflex agent\\n\",\"\\n\",\"Run the next cell. All asserts should pass after you implement `simple_reflex_agent`.\\n\"]},{\"cell_type\":\"code\",\"execution_count\":null,\"id\":\"25a354a9\",\"metadata\":{\"id\":\"25a354a9\"},\"outputs\":[],\"source\":[\"# Unit tests (do not modify)\\n\",\"assert simple_reflex_agent((\\\"Left\\\", \\\"Dirty\\\")) == \\\"Suck\\\"\\n\",\"assert simple_reflex_agent((\\\"Right\\\", \\\"Dirty\\\")) == \\\"Suck\\\"\\n\",\"assert simple_reflex_agent((\\\"Left\\\", \\\"Clean\\\")) in (\\\"Right\\\",)   # move to other\\n\",\"assert simple_reflex_agent((\\\"Right\\\", \\\"Clean\\\")) in (\\\"Left\\\",)   # move to other\\n\",\"print(\\\"simple_reflex_agent tests passed\\\")\\n\"]},{\"cell_type\":\"markdown\",\"id\":\"3b7d0a7d\",\"metadata\":{\"id\":\"3b7d0a7d\"},\"source\":[\"## 3. TODO: Model-Based Reflex Agent (Internal State)\\n\",\"\\n\",\"A model-based reflex agent maintains an **internal state** that summarizes what it knows.\\n\",\"\\n\",\"We will store state as:\\n\",\"\\n\",\"```python\\n\",\"state = { \\\"dirty\\\": { \\\"Left\\\": \\\"?\\\", \\\"Right\\\": \\\"?\\\" } }\\n\",\"```\\n\",\"\\n\",\"Where:\\n\",\"- `\\\"?\\\"` means unknown\\n\",\"- the agent updates the entry for its current location based on the percept\\n\",\"\\n\",\"Decision logic (suggested):\\n\",\"1. If current square is `Dirty` → `Suck`\\n\",\"2. Else, if the other square is not known to be `Clean` → move to it\\n\",\"3. Else → `NoOp`\\n\",\"\\n\",\"**Important:** In this notebook we return `(action, state)` so state can persist across calls.\\n\"]},{\"cell_type\":\"code\",\"execution_count\":null,\"id\":\"026c319c\",\"metadata\":{\"id\":\"026c319c\"},\"outputs\":[],\"source\":[\"def mb_reflex_agent(percept: Percept, state: Optional[dict] = None) -> Tuple[Action, dict]:\\n\",\"    \\\"\\\"\\\"Model-based reflex agent for vacuum world.\\n\",\"\\n\",\"    TODO:\\n\",\"    - Initialize state if None\\n\",\"    - Update state using the current percept\\n\",\"    - Choose action using the decision logic above\\n\",\"    - Return (action, updated_state)\\n\",\"    \\\"\\\"\\\"\\n\",\"    # TODO: unpack percept into loc, status\\n\",\"    # TODO: initialize state = {'dirty': {'Left': '?', 'Right': '?'}} if needed\\n\",\"    # TODO: update state['dirty'][loc] = status\\n\",\"    # TODO: choose action using state and percept\\n\",\"    raise NotImplementedError\\n\"]},{\"cell_type\":\"markdown\",\"id\":\"60044c1c\",\"metadata\":{\"id\":\"60044c1c\"},\"source\":[\"### Quick tests for your model-based reflex agent\\n\",\"\\n\",\"Run the next cell. All asserts should pass after you implement `mb_reflex_agent`.\\n\"]},{\"cell_type\":\"code\",\"execution_count\":null,\"id\":\"0f04c7cd\",\"metadata\":{\"id\":\"0f04c7cd\"},\"outputs\":[],\"source\":[\"# Unit tests (do not modify)\\n\",\"a, st = mb_reflex_agent((\\\"Left\\\", \\\"Clean\\\"), None)\\n\",\"assert \\\"dirty\\\" in st and \\\"Left\\\" in st[\\\"dirty\\\"] and \\\"Right\\\" in st[\\\"dirty\\\"]\\n\",\"assert st[\\\"dirty\\\"][\\\"Left\\\"] == \\\"Clean\\\"\\n\",\"\\n\",\"a, st2 = mb_reflex_agent((\\\"Left\\\", \\\"Dirty\\\"), st)\\n\",\"assert a == \\\"Suck\\\"\\n\",\"assert st2[\\\"dirty\\\"][\\\"Left\\\"] == \\\"Dirty\\\"\\n\",\"\\n\",\"print(\\\"mb_reflex_agent basic tests passed\\\")\\n\"]},{\"cell_type\":\"markdown\",\"id\":\"e208dc2e\",\"metadata\":{\"id\":\"e208dc2e\"},\"source\":[\"## 4. Episode Runner\\n\",\"\\n\",\"We will run an agent in the environment for `max_steps` steps.\\n\",\"\\n\",\"- Simple reflex agent returns only an action.\\n\",\"- Model-based agent returns `(action, state)`.\\n\"]},{\"cell_type\":\"code\",\"execution_count\":null,\"id\":\"556abf72\",\"metadata\":{\"id\":\"556abf72\"},\"outputs\":[],\"source\":[\"def run_episode(env: VacuumWorld, agent_type: str = \\\"simple\\\", verbose: bool = False) -> dict:\\n\",\"    total_reward = 0.0\\n\",\"    trajectory = []\\n\",\"    state = None\\n\",\"\\n\",\"    while True:\\n\",\"        percept = env.percept()\\n\",\"\\n\",\"        if agent_type == \\\"simple\\\":\\n\",\"            action = simple_reflex_agent(percept)\\n\",\"        elif agent_type == \\\"model_based\\\":\\n\",\"            action, state = mb_reflex_agent(percept, state)\\n\",\"        else:\\n\",\"            raise ValueError(\\\"agent_type must be 'simple' or 'model_based'\\\")\\n\",\"\\n\",\"        next_percept, reward, done = env.step(action)\\n\",\"        total_reward += reward\\n\",\"\\n\",\"        trajectory.append({\\n\",\"            \\\"t\\\": env.step_count,\\n\",\"            \\\"percept\\\": percept,\\n\",\"            \\\"action\\\": action,\\n\",\"            \\\"reward\\\": reward,\\n\",\"            \\\"dirt\\\": dict(env.dirt),\\n\",\"            \\\"loc\\\": env.agent_loc,\\n\",\"            \\\"state\\\": None if state is None else {\\\"dirty\\\": dict(state[\\\"dirty\\\"])},\\n\",\"        })\\n\",\"\\n\",\"        if verbose:\\n\",\"            print(f\\\"t={env.step_count:2d} percept={percept} action={action:5s} \\\"\\n\",\"                  f\\\"dirt={env.dirt} reward={reward:.2f} state={trajectory[-1]['state']}\\\")\\n\",\"\\n\",\"        if done:\\n\",\"            break\\n\",\"\\n\",\"    return {\\\"total_reward\\\": total_reward, \\\"trajectory\\\": trajectory}\\n\"]},{\"cell_type\":\"markdown\",\"id\":\"0d66973b\",\"metadata\":{\"id\":\"0d66973b\"},\"source\":[\"## 5. TODO: Experiment and Compare\\n\",\"\\n\",\"Implement `evaluate()` to compare average total reward for both agents across many random episodes.\\n\",\"\\n\",\"Requirements:\\n\",\"- Use the same initial environment for both agents per episode.\\n\",\"- Return a dictionary with keys `\\\"simple\\\"` and `\\\"model_based\\\"`, each mapped to a list of total rewards.\\n\"]},{\"cell_type\":\"code\",\"execution_count\":null,\"id\":\"c4d988b5\",\"metadata\":{\"id\":\"c4d988b5\"},\"outputs\":[],\"source\":[\"def evaluate(n_episodes: int = 200, p_dirty: float = 0.5, max_steps: int = 20, seed: int = 0) -> dict:\\n\",\"    \\\"\\\"\\\"Run many episodes and collect rewards for both agents.\\n\",\"\\n\",\"    TODO:\\n\",\"    - Set random seed\\n\",\"    - For each episode:\\n\",\"        * Create one random world env1\\n\",\"        * Create env2 as a copy of env1 (same dirt + same start location)\\n\",\"        * Run both agents\\n\",\"        * Append total rewards into results dict\\n\",\"    - Return results\\n\",\"    \\\"\\\"\\\"\\n\",\"    # TODO\\n\",\"    raise NotImplementedError\\n\"]},{\"cell_type\":\"markdown\",\"id\":\"5434a614\",\"metadata\":{\"id\":\"5434a614\"},\"source\":[\"### Run your evaluation\\n\",\"\\n\",\"After implementing `evaluate()`, run the next cell to see average performance.\\n\"]},{\"cell_type\":\"code\",\"execution_count\":null,\"id\":\"49b6d0a8\",\"metadata\":{\"id\":\"49b6d0a8\"},\"outputs\":[],\"source\":[\"results = evaluate(n_episodes=500, p_dirty=0.5, max_steps=20, seed=42)\\n\",\"avg_simple = sum(results[\\\"simple\\\"]) / len(results[\\\"simple\\\"])\\n\",\"avg_mb = sum(results[\\\"model_based\\\"]) / len(results[\\\"model_based\\\"])\\n\",\"avg_simple, avg_mb\\n\"]},{\"cell_type\":\"markdown\",\"id\":\"03714a4d\",\"metadata\":{\"id\":\"03714a4d\"},\"source\":[\"## 6. Plot the Results\\n\",\"\\n\",\"Run the next cell to visualize distributions.\\n\"]},{\"cell_type\":\"code\",\"execution_count\":null,\"id\":\"0b42994a\",\"metadata\":{\"id\":\"0b42994a\"},\"outputs\":[],\"source\":[\"import matplotlib.pyplot as plt\\n\",\"\\n\",\"plt.figure()\\n\",\"plt.hist(results[\\\"simple\\\"], bins=20, alpha=0.7, label=\\\"Simple Reflex\\\")\\n\",\"plt.hist(results[\\\"model_based\\\"], bins=20, alpha=0.7, label=\\\"Model-Based Reflex\\\")\\n\",\"plt.xlabel(\\\"Total reward per episode\\\")\\n\",\"plt.ylabel(\\\"Count\\\")\\n\",\"plt.legend()\\n\",\"plt.show()\\n\"]},{\"cell_type\":\"markdown\",\"id\":\"87dfe6dc\",\"metadata\":{\"id\":\"87dfe6dc\"},\"source\":[\"## 7. Written Questions (Answer in Markdown)\\n\",\"\\n\",\"### Q1. Scaling\\n\",\"Why does an explicit lookup table representation of the agent function become infeasible as environments grow?\\n\",\"\\n\",\"**Answer:** *(write your answer here)*\\n\",\"\\n\",\"### Q2. Partial Observability\\n\",\"Explain, in your own words, why partial observability motivates internal state.\\n\",\"\\n\",\"**Answer:** *(write your answer here)*\\n\",\"\\n\",\"### Q3. Model-Based vs Simple Reflex\\n\",\"In Vacuum World as implemented here, when (if ever) does internal state help? What changes to the environment would make internal state more important?\\n\",\"\\n\",\"**Answer:** *(write your answer here)*\\n\",\"\\n\",\"### Q4. Connection to ML\\n\",\"Which learning agent component (performance element, learning element, critic, problem generator) is most directly implemented by ML algorithms, and why?\\n\",\"\\n\",\"**Answer:** *(write your answer here)*\\n\"]}],\"metadata\":{\"colab\":{\"provenance\":[]}},\"nbformat\":4,\"nbformat_minor\":5}",
   "id": "97b91c462aa6fa07"
  },
  {
   "cell_type": "markdown",
   "id": "8984e445",
   "metadata": {
    "id": "8984e445"
   },
   "source": [
    "## 0. Setup\n",
    "\n",
    "Run the next cell to import libraries. Do not add new external packages.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "a2b1cbb2",
   "metadata": {
    "id": "a2b1cbb2",
    "ExecuteTime": {
     "end_time": "2026-01-20T22:46:54.911255Z",
     "start_time": "2026-01-20T22:46:54.878467Z"
    }
   },
   "source": [
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Tuple, Optional, List\n",
    "\n",
    "# Reproducibility\n",
    "random.seed(0)\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "5ee1f897",
   "metadata": {
    "id": "5ee1f897"
   },
   "source": [
    "## 1. Vacuum World Environment\n",
    "\n",
    "The environment has:\n",
    "- two locations: `Left`, `Right`\n",
    "- each location is `Clean` or `Dirty`\n",
    "- the agent occupies one location at a time\n",
    "\n",
    "A **percept** is a tuple `(location, status_of_current_square)`.\n",
    "\n",
    "Run the next cell to define the environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "5c74674e",
   "metadata": {
    "id": "5c74674e",
    "ExecuteTime": {
     "end_time": "2026-01-20T22:46:57.525608Z",
     "start_time": "2026-01-20T22:46:57.508529Z"
    }
   },
   "source": [
    "Location = str  # 'Left' or 'Right'\n",
    "Status   = str    # 'Clean' or 'Dirty'\n",
    "Action   = str    # 'Left', 'Right', 'Suck', 'NoOp'\n",
    "Percept  = Tuple[Location, Status]\n",
    "\n",
    "@dataclass\n",
    "class VacuumWorld:\n",
    "    dirt: Dict[Location, Status]\n",
    "    agent_loc: Location\n",
    "    step_count: int = 0\n",
    "    max_steps: int = 20\n",
    "\n",
    "    def percept(self) -> Percept:\n",
    "        return (self.agent_loc, self.dirt[self.agent_loc])\n",
    "\n",
    "    def step(self, action: Action) -> Tuple[Percept, float, bool]:\n",
    "        self.step_count += 1\n",
    "\n",
    "        if action == \"Suck\":\n",
    "            self.dirt[self.agent_loc] = \"Clean\"\n",
    "        elif action == \"Left\":\n",
    "            self.agent_loc = \"Left\"\n",
    "        elif action == \"Right\":\n",
    "            self.agent_loc = \"Right\"\n",
    "        elif action == \"NoOp\":\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(\"Unknown action\")\n",
    "\n",
    "        clean_count = sum(1 for s in self.dirt.values() if s == \"Clean\")\n",
    "        move_cost = 0.1 if action in (\"Left\", \"Right\") else 0.0\n",
    "        reward = float(clean_count) - move_cost\n",
    "\n",
    "        done = self.step_count >= self.max_steps\n",
    "        return self.percept(), reward, done\n",
    "\n",
    "\n",
    "def random_world(p_dirty: float = 0.5, start_loc: Optional[Location] = None, max_steps: int = 20):\n",
    "    dirt = {\n",
    "        \"Left\": \"Dirty\" if random.random() < p_dirty else \"Clean\",\n",
    "        \"Right\": \"Dirty\" if random.random() < p_dirty else \"Clean\",\n",
    "    }\n",
    "    if start_loc is None:\n",
    "        start_loc = random.choice([\"Left\", \"Right\"])\n",
    "    return VacuumWorld(dirt=dirt, agent_loc=start_loc, max_steps=max_steps)\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "82263d9e",
   "metadata": {
    "id": "82263d9e"
   },
   "source": [
    "## 2. TODO: Simple Reflex Agent (Condition–Action Rules)\n",
    "\n",
    "Implement the classic simple reflex logic:\n",
    "\n",
    "- If current square is `Dirty` → `Suck`\n",
    "- Else move to the other square (`Left` ↔ `Right`)\n",
    "\n",
    "**Important:** This agent ignores percept history. It reacts to the current percept only.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "58e195a5",
   "metadata": {
    "id": "58e195a5",
    "ExecuteTime": {
     "end_time": "2026-01-20T22:47:02.050703Z",
     "start_time": "2026-01-20T22:47:02.027976Z"
    }
   },
   "source": [
    "def simple_reflex_agent(percept: Percept) -> Action:\n",
    "    loc, status = percept\n",
    "    if status == \"Dirty\":\n",
    "        return \"Suck\"\n",
    "    return \"Right\" if loc == \"Left\" else \"Left\"\n"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "12127e7d",
   "metadata": {
    "id": "12127e7d"
   },
   "source": [
    "### Quick tests for your simple reflex agent\n",
    "\n",
    "Run the next cell. All asserts should pass after you implement `simple_reflex_agent`.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "25a354a9",
   "metadata": {
    "id": "25a354a9",
    "ExecuteTime": {
     "end_time": "2026-01-20T22:47:03.719587Z",
     "start_time": "2026-01-20T22:47:03.673008Z"
    }
   },
   "source": [
    "# Unit tests (do not modify)\n",
    "assert simple_reflex_agent((\"Left\", \"Dirty\")) == \"Suck\"\n",
    "assert simple_reflex_agent((\"Right\", \"Dirty\")) == \"Suck\"\n",
    "assert simple_reflex_agent((\"Left\", \"Clean\")) in (\"Right\",)   # move to other\n",
    "assert simple_reflex_agent((\"Right\", \"Clean\")) in (\"Left\",)   # move to other\n",
    "print(\"simple_reflex_agent tests passed\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simple_reflex_agent tests passed\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "3b7d0a7d",
   "metadata": {
    "id": "3b7d0a7d"
   },
   "source": [
    "## 3. TODO: Model-Based Reflex Agent (Internal State)\n",
    "\n",
    "A model-based reflex agent maintains an **internal state** that summarizes what it knows.\n",
    "\n",
    "We will store state as:\n",
    "\n",
    "```python\n",
    "state = { \"dirty\": { \"Left\": \"?\", \"Right\": \"?\" } }\n",
    "```\n",
    "\n",
    "Where:\n",
    "- `\"?\"` means unknown\n",
    "- the agent updates the entry for its current location based on the percept\n",
    "\n",
    "Decision logic (suggested):\n",
    "1. If current square is `Dirty` → `Suck`\n",
    "2. Else, if the other square is not known to be `Clean` → move to it\n",
    "3. Else → `NoOp`\n",
    "\n",
    "**Important:** In this notebook we return `(action, state)` so state can persist across calls.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "026c319c",
   "metadata": {
    "id": "026c319c",
    "ExecuteTime": {
     "end_time": "2026-01-20T22:47:07.699629Z",
     "start_time": "2026-01-20T22:47:07.678130Z"
    }
   },
   "source": [
    "def mb_reflex_agent(percept: Percept, state: Optional[dict] = None):\n",
    "    loc, status = percept\n",
    "\n",
    "    if state is None:\n",
    "        state = {\"dirty\": {\"Left\": \"?\", \"Right\": \"?\"}}\n",
    "\n",
    "    state[\"dirty\"][loc] = status\n",
    "\n",
    "    if status == \"Dirty\":\n",
    "        return \"Suck\", state\n",
    "\n",
    "    other = \"Right\" if loc == \"Left\" else \"Left\"\n",
    "    if state[\"dirty\"][other] != \"Clean\":\n",
    "        return other, state\n",
    "\n",
    "    return \"NoOp\", state\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "60044c1c",
   "metadata": {
    "id": "60044c1c"
   },
   "source": [
    "### Quick tests for your model-based reflex agent\n",
    "\n",
    "Run the next cell. All asserts should pass after you implement `mb_reflex_agent`.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "0f04c7cd",
   "metadata": {
    "id": "0f04c7cd",
    "ExecuteTime": {
     "end_time": "2026-01-20T22:47:10.269847Z",
     "start_time": "2026-01-20T22:47:10.230686Z"
    }
   },
   "source": [
    "# Unit tests (do not modify)\n",
    "a, st = mb_reflex_agent((\"Left\", \"Clean\"), None)\n",
    "assert \"dirty\" in st and \"Left\" in st[\"dirty\"] and \"Right\" in st[\"dirty\"]\n",
    "assert st[\"dirty\"][\"Left\"] == \"Clean\"\n",
    "\n",
    "a, st2 = mb_reflex_agent((\"Left\", \"Dirty\"), st)\n",
    "assert a == \"Suck\"\n",
    "assert st2[\"dirty\"][\"Left\"] == \"Dirty\"\n",
    "\n",
    "print(\"mb_reflex_agent basic tests passed\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mb_reflex_agent basic tests passed\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "e208dc2e",
   "metadata": {
    "id": "e208dc2e"
   },
   "source": [
    "## 4. Episode Runner\n",
    "\n",
    "We will run an agent in the environment for `max_steps` steps.\n",
    "\n",
    "- Simple reflex agent returns only an action.\n",
    "- Model-based agent returns `(action, state)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "556abf72",
   "metadata": {
    "id": "556abf72",
    "ExecuteTime": {
     "end_time": "2026-01-20T22:47:15.322602Z",
     "start_time": "2026-01-20T22:47:15.307441Z"
    }
   },
   "source": [
    "def run_episode(env: VacuumWorld, agent_type: str = \"simple\", verbose: bool = False) -> dict:\n",
    "    total_reward = 0.0\n",
    "    trajectory = []\n",
    "    state = None\n",
    "\n",
    "    while True:\n",
    "        percept = env.percept()\n",
    "\n",
    "        if agent_type == \"simple\":\n",
    "            action = simple_reflex_agent(percept)\n",
    "        elif agent_type == \"model_based\":\n",
    "            action, state = mb_reflex_agent(percept, state)\n",
    "        else:\n",
    "            raise ValueError(\"agent_type must be 'simple' or 'model_based'\")\n",
    "\n",
    "        next_percept, reward, done = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        trajectory.append({\n",
    "            \"t\": env.step_count,\n",
    "            \"percept\": percept,\n",
    "            \"action\": action,\n",
    "            \"reward\": reward,\n",
    "            \"dirt\": dict(env.dirt),\n",
    "            \"loc\": env.agent_loc,\n",
    "            \"state\": None if state is None else {\"dirty\": dict(state[\"dirty\"])},\n",
    "        })\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"t={env.step_count:2d} percept={percept} action={action:5s} \"\n",
    "                  f\"dirt={env.dirt} reward={reward:.2f} state={trajectory[-1]['state']}\")\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return {\"total_reward\": total_reward, \"trajectory\": trajectory}\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "0d66973b",
   "metadata": {
    "id": "0d66973b"
   },
   "source": [
    "## 5. TODO: Experiment and Compare\n",
    "\n",
    "Implement `evaluate()` to compare average total reward for both agents across many random episodes.\n",
    "\n",
    "Requirements:\n",
    "- Use the same initial environment for both agents per episode.\n",
    "- Return a dictionary with keys `\"simple\"` and `\"model_based\"`, each mapped to a list of total rewards.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "c4d988b5",
   "metadata": {
    "id": "c4d988b5",
    "ExecuteTime": {
     "end_time": "2026-01-20T22:49:28.481689Z",
     "start_time": "2026-01-20T22:49:28.473446Z"
    }
   },
   "source": [
    "def evaluate(n_episodes: int = 200, p_dirty: float = 0.5, max_steps: int = 20, seed: int = 0):\n",
    "    random.seed(seed)\n",
    "    results = {\"simple\": [], \"model_based\": []}\n",
    "\n",
    "    for _ in range(n_episodes):\n",
    "        env1 = random_world(p_dirty=p_dirty, max_steps=max_steps)\n",
    "        env2 = VacuumWorld(\n",
    "            dirt=dict(env1.dirt),\n",
    "            agent_loc=env1.agent_loc,\n",
    "            max_steps=max_steps\n",
    "        )\n",
    "\n",
    "        results[\"simple\"].append(run_episode(env1, \"simple\")[\"total_reward\"])\n",
    "        results[\"model_based\"].append(run_episode(env2, \"model_based\")[\"total_reward\"])\n",
    "\n",
    "    return results\n"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "5434a614",
   "metadata": {
    "id": "5434a614"
   },
   "source": [
    "### Run your evaluation\n",
    "\n",
    "After implementing `evaluate()`, run the next cell to see average performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "49b6d0a8",
   "metadata": {
    "id": "49b6d0a8",
    "ExecuteTime": {
     "end_time": "2026-01-20T22:49:30.923260Z",
     "start_time": "2026-01-20T22:49:30.883007Z"
    }
   },
   "source": [
    "results = evaluate(n_episodes=500, p_dirty=0.5, max_steps=20, seed=42)\n",
    "avg_simple = sum(results[\"simple\"]) / len(results[\"simple\"])\n",
    "avg_mb = sum(results[\"model_based\"]) / len(results[\"model_based\"])\n",
    "avg_simple, avg_mb\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37.35419999999999, 39.154)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "id": "03714a4d",
   "metadata": {
    "id": "03714a4d"
   },
   "source": [
    "## 6. Plot the Results\n",
    "\n",
    "Run the next cell to visualize distributions.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "0b42994a",
   "metadata": {
    "id": "0b42994a",
    "ExecuteTime": {
     "end_time": "2026-01-20T22:49:36.125790Z",
     "start_time": "2026-01-20T22:49:36.059504Z"
    }
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(results[\"simple\"], bins=20, alpha=0.7, label=\"Simple Reflex\")\n",
    "plt.hist(results[\"model_based\"], bins=20, alpha=0.7, label=\"Model-Based Reflex\")\n",
    "plt.xlabel(\"Total reward per episode\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "id": "87dfe6dc",
   "metadata": {
    "id": "87dfe6dc"
   },
   "source": [
    "## 7. Written Questions (Answer in Markdown)\n",
    "\n",
    "### Q1. Scaling\n",
    "Why does an explicit lookup table representation of the agent function become infeasible as environments grow?\n",
    "\n",
    "**Answer:** *An explicit lookup table becomes infeasible because the number of possible percept histories grows exponentially as the environment becomes larger, making storage and computation impractical.*\n",
    "\n",
    "### Q2. Partial Observability\n",
    "Explain, in your own words, why partial observability motivates internal state.\n",
    "\n",
    "**Answer:** *Partial observability motivates internal state because the agent cannot rely solely on current percepts and must remember or infer information about unobserved parts of the environment.*\n",
    "\n",
    "### Q3. Model-Based vs Simple Reflex\n",
    "In Vacuum World as implemented here, when (if ever) does internal state help? What changes to the environment would make internal state more important?\n",
    "\n",
    "**Answer:** *Internal state helps once both squares are clean by preventing unnecessary movement. In larger or noisier environments, internal state becomes essential for good performance.*\n",
    "\n",
    "### Q4. Connection to ML\n",
    "Which learning agent component (performance element, learning element, critic, problem generator) is most directly implemented by ML algorithms, and why?\n",
    "\n",
    "**Answer:** *Machine learning most directly implements the performance element, since it learns how to map percepts to actions based on experience.*\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
